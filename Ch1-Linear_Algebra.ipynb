{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 - Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vector spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Introduction\n",
    "- A set $V$ is called a vector space over field $R$ if it is closed under summation and scalar multiplication, $\\textit{i.e.,}$ $\\alpha v_1 + \\beta v_2 \\in V\\ \\forall v_1, v_2 \\in V$ and $ \\alpha, \\beta \\in R$.\n",
    "- A set $W \\subset V$ is called a subspace if $0 \\in W$ and $W$ is closed under summation and scalar multiplication\n",
    "- **Span(S)** - The subspace formed by linear combination of a set $S$ of vectors $\\{v_1, v_2, \\ldots, v_k\\} \\in V$\n",
    "$$Span(S) = \\{ v \\in V | v = \\sum_{i=1}^k \\alpha_i v_i \\}$$\n",
    "- Set $S = \\{v_1, v_2, \\ldots, v_k\\}$ is **linearly independent** when $\\sum_{i=1}^k \\alpha_i v_i = 0 \\iff \\alpha_i = 0\\ \\forall i$, $\\textit{i.e.,}$ none of the vectors in $S$ can be expressed as a linear combination of the other vectors in $S$.\n",
    "- **Basis vectors** - maximal set of linearly independent vectors $S = \\{v_1, v_2, \\ldots, v_k\\}$ that span the vector space $Span(S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Properties of basis\n",
    "A vector space $V$ can have infinitely many bases. For any two bases $B$ and $B'$,\n",
    "- $B$ and $B'$ contain same number of vectors called dimension of $V$\n",
    "- Any vector $v \\in V$ can be uniquely expressed as a linear combination of basis vectors in $B$, $\\textit{i.e.,}$ $ v = \\sum_i=1^k \\alpha_i v_i$.\n",
    "- Basis vectors in $B'$ can be expressed as a linear combination of basis vectors in $B$, $\\textit{i.e.,}$ $b'_i = \\sum_j=1^k \\alpha_{ij} v_j$. Coefficients $\\alpha_{ij}$ form the *basis transformation matrix A*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Inner product or Dot product or Scalar product\n",
    "a) Inner product on vector space $V$ is defined as $<.,.> : V \\times V \\in R$ such that\n",
    "- $<u, \\alpha v + \\beta w> = \\alpha <u,v> + \\beta <u,w>$ (linear)\n",
    "- $<u,v> = <v,u>$ (symmetric)\n",
    "- $<v,v>\\ \\ge\\ 0$ and $<v,v> = 0 \\iff v = 0$ (positive definite)\n",
    "\n",
    "b) **Norm**: Length of vector $v$, $|v| = \\sqrt{<v,v>}$\n",
    "\n",
    "c) **Metric**: For measuring lengths and distances, $d(v,w) = |v-w| = \\sqrt{<v-w, v-w>}$\n",
    "\n",
    "d) Two vectors $v$ and $w$ are orthogonal $\\iff <v,w> = 0$. (Basis vectors do not have to be necessarily orthogonal.)\n",
    "\n",
    "e) For $ V \\in R^n$ and canonical basis $B = I_n$, the inner product is defined as $<x,y> = x^Ty = \\sum_{i=1}^n x_i y_i$. L2-norm or Euclidean norm is given by $<x,x> = x^Tx = \\sqrt{x_1^2 + \\ldots + x_n^2}$ \n",
    "\n",
    "***Note***\n",
    "- A vector space with a metric (to measure lengths / distances) is called *metric space*.\n",
    "- A vector space whose metric is induced by inner product is called *Hilbert space*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Kronecker product and stack of matrix\n",
    "1) **Kronecker product** of two matrices $A \\in R^{m \\times n}$ and $B \\in R^{k \\times l}$ yields a matrix $C \\in R^{mk \\times nl}$ where $B$ is multiplied by each coefficient of $A$ and stacked.\n",
    "\n",
    "2) **Stack** of matrix $A \\in R^{m \\times n}$ yields $C \\in R^{mn}$ by stacking columns of $A$ vertically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear transformations and matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear transformation $L$ between two vector spaces $V$ and $W$ is a map $L : V \\rightarrow W$ such that linearity (summation and scaling) holds.\n",
    "$$ L(x+y) = L(x) + L(y) \\forall x,y \\in V $$\n",
    "$$L(\\alpha x) = \\alpha L(x)$$\n",
    "- Linear transformation is defined by a matrix $A$ which consists of $L$ applied to all basis vectors, $\\textit{i.e.,}\\ A =[L(e_1), \\ldots, L(e_n)] \\in R^{m \\times n}$\n",
    "- Since linear transformations are represented as matrices, linear algebra studies properties of matrices\n",
    "- Unlike inner product of vectors which doesn't yield another vector, product of matrices yield another matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Groups\n",
    "A set of linear transformations form a group $G$ with an operation $\\circ : G \\times G \\rightarrow G$ such that\n",
    "- application of operation results in a matrix within the group, $g_1 \\circ g_2 \\in G\\ \\forall g_1, g_2 \\in G$\n",
    "- associativity holds, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)\\ \\forall g_1, g_2, g_3 \\in G$\n",
    "- an identity transformation $e$ exists, $\\exists e \\in G, e \\circ g = g \\circ e = g\\ \\forall g \\in G$\n",
    "- inverse transformation $g^{-1}$ exists, $\\exists g^{-1} \\in G, g^{-1} \\circ g = g \\circ g^{-1} = e\\ \\forall g \\in G$\n",
    "\n",
    "All invertible or non-singular real matrices, $A$, have $det(A) \\neq 0$ and form a group (general linear group) under matrix multiplication.\n",
    "\n",
    "Groups can also have a matrix representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Affine group A(n)\n",
    "Affine transformation $L : R^n \\rightarrow R^n$ is defined by a matrix $A$ such that $L(x) = Ax + b$ where $A$ must be invertible. Note that this is not a linear transformation unless $b=0$. \n",
    "\n",
    "More precisely, represent $x$ as homogeneous coordinates, $x \\in R^{n+1}$, to make $L$ a linear map, such that,\n",
    "$$ \n",
    "L ( \\left[ {\\begin{array}{c}\n",
    "   x \\\\\n",
    "   1 \\\\\n",
    "  \\end{array} } \\right] ) = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   A & b \\\\\n",
    "   0 & 1 \\\\\n",
    "  \\end{array} } \\right]  \n",
    "  \\left[ {\\begin{array}{c}\n",
    "   x \\\\\n",
    "   1 \\\\\n",
    "  \\end{array} } \\right]  \n",
    "$$\n",
    "\n",
    "and $L : R^{n+1} \\rightarrow R^{n+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Orthogonal group R(n)\n",
    "A matrix $ R \\in \\mathcal{M}(n) $ is called orthogonal if it preserves the inner product ($\\mathcal{M}(n)$ denotes all $n \\times n$ real matrices).\n",
    "$$ <Rx,Ry>\\ =\\ <x,y>\\ \\forall x,y \\in R^n$$\n",
    "$$ <Rx,Ry>\\ =\\ xR^TRy\\ =\\ <x,y>\\ \\forall x,y \\in R^n$$\n",
    "Hence, we must have $R^TR = RR^T = I$. Since, the scalar product of the transformed vectors equals the scalar product of the actual vectors, orthogonal transformations preserves the angles. Note that scalar product between two vectors gives the angle between the vectors. \n",
    "\n",
    "Orthogonal transformations generally represent rotations and mirroring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Euclidean group E(n)\n",
    "A Euclidean transformation $E$ is defined by a rotation $R$ (orthogonal matrix) and a translation $T \\in R^n$ (vector).\n",
    "$$E(x) = Rx + T$$\n",
    "In homogeneous coordinates, \n",
    "$$ \n",
    "E(n) = \\left[ {\\begin{array}{cc}\n",
    "   R & T \\\\\n",
    "   0 & 1 \\\\\n",
    "  \\end{array} } \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Range and Null space\n",
    "- Range or span of a transformation $A$ is defined as subspace of $R^m$ that can be reached by $A$. Range is given by the span of the column vectors of $A$.\n",
    "- Null space or kernel of $A$ are the subset of vectors $\\in R^n$ that are mapped to $0$ by $A$.\n",
    "- Range and kernel are complementary. $dim(range(A)) + dim(kernel(A)) = n$.\n",
    "- Useful to study solution of linear equations $Ax = b$\n",
    "    * $Ax = b$ has a solution only if $b \\in range(A)$\n",
    "    * It has unique solution only if $kernel(A) = {0}$, $\\textit{i.e.,}$ only zero vector is in the null space\n",
    "    * If $kernel(A)$ has other vectors besides the zero vector, then there are many solutions for $Ax = b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Rank of a matrix\n",
    "Rank of a matrix $A \\in R^{m \\times n}$\n",
    "- $Rank(A) = dim(range(A))$\n",
    "- $Rank(A) = n - dim(kernel(A))$\n",
    "- $ 0 \\leq rank(A) \\leq min(m,n)$\n",
    "- $Rank(A)$ = maximum number of linearly independent row (or column) vectors of $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Eigenvalues and Eigenvectors\n",
    "If multiplying a vector by a transformation matrix does nothing but scale the vector, then the vector is called the eigenvector and the scaling factor is called eigenvalue. Formally,\n",
    " let $A \\in C^{n \\times n}$, $v \\in C^n$ and $\\lambda \\in C$. If \n",
    " $$ Av = \\lambda v $$ \n",
    " then, $v$ is called the (right) eigenvector and $\\lambda$ is called the eigenvalue of $A$. \n",
    " \n",
    "If $v^TA = \\lambda v^T$, $v$ is the left eigenvector of $A$.  \n",
    " \n",
    "Spectrum $\\sigma(A)$ is the set of its eigenvalues.\n",
    "\n",
    "Since, $Av = \\lambda v$, we can write $(A-\\lambda I)v = 0$. If $(A-\\lambda I)$ is non-singular, $\\textit{i.e.}$ $det(A-\\lambda I) \\neq 0$, then only way to solve $(A-\\lambda I)v = 0$ is when $v = \\bar{0}$. If we need non-zero $v$, then $det(A-\\lambda I) = 0$ which forms the characteristic polynomial equation.\n",
    " \n",
    " #### Properties:\n",
    " Let $A \\in R^{n \\times n}$.\n",
    " - If $Av = \\lambda v$, then there also exists a left eigenvector $\\eta \\in R^n$. Hence $\\sigma(A) = \\sigma(A^T)$.\n",
    " - Eigenvectors associated with unique eigenvalues are linearly independent\n",
    " - det(A) = product of all eigenvalues since eigenvalues are the roots of the characteristic polynomial equation $det(A-\\lambda I) = 0$.\n",
    " - If $B = PAP^{-1}$ for some non-singular matrix $P$, then $\\sigma(B) = \\sigma(A)$.\n",
    " - If $\\lambda \\in C$ is an eigenvalue, then its conjugate $\\bar{\\lambda}$ is also an eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Symmetric matrices\n",
    "- A matrix $S \\in R^{n \\times n}$ is symmetric if $S = S^T$.\n",
    "- S is positive semi-definite if $x^TSx \\geq 0\\ \\forall x \\neq 0$.\n",
    "- S is positive definite if $x^TSx \\gt 0\\ \\forall x \\neq 0$.\n",
    "\n",
    "#### Properties:\n",
    "- All eigenvalues of $S$ are real, $\\sigma(S) \\subset R$.\n",
    "- Eigenvectors $v_i$ and $v_j$ of $S$ corresponding to distinct eigenvalues are orthogonal.\n",
    "- If $V = (v_1, v_2, \\ldots, v_n)$ be the set of eigenvectors of $S$ ($V$ is orthogonal matrix) and $\\Lambda = diag(\\{\\lambda_1, \\ldots, \\lambda_n\\})$ be the corresponding eigenvalues (in a diagonal matrix), $S = V \\Lambda V^T$.\n",
    "- If S is positive (semi-)definite, all eigenvalues are positive (nonnegative).\n",
    "- If $\\lambda_1$ and $\\lambda_n$ are the largest and smallest eigenvalue of $S$, then $\\lambda_1 = max_{|x|=1} <x,Sx>$ and $\\lambda_1 = min_{|x|=1} <x,Sx>$ where $|x|=1$ denotes vector in unit sphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Norm of matrix\n",
    "There are different ways to define a norm of a matrix $A \\in R^{m \\times n}$.\n",
    "\n",
    "- induced L2-norm $||A||_2 = max_{|x|=1} |Ax|_2 = max_{|x|=1} \\sqrt{<x,A^TAx>}$. Since $A^TA$ is symmetric, $||A||_2 = \\sigma_1$, the largest eigenvalue of $A^TA$.\n",
    "- Frobenius norm $||A||_f = \\sqrt{\\sum_{i,j} a_{ij}} = \\sqrt{trace(A^TA)}$. Since $A^TA$ is symmetric and positive semi-definite, it can be diagonalized as $V diag(\\sigma_1^2 + \\ldots + \\sigma_n^2) V^T$ (where $\\sigma_i^2 \\geq 0$), then $||A||_f = \\sqrt{\\sigma_1^2 + \\ldots + \\sigma_n^2}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Skew-symmetric matrices\n",
    "- A matrix $A \\in R^{n \\times n}$ is symmetric if $A^T = -A$. (diagonal will be zero)\n",
    "- All eigenvalues of $A$ are either zero or purely imaginary.\n",
    "- $A = V \\Lambda V^T$ where $\\Lambda$ is a block diagonal matrix with $2 \\times 2$ smaller skew symmetric matrices. \n",
    "- Rank of $A$ is even\n",
    "\n",
    "A useful skew-symmetric matrix in computer vision is given by\n",
    "$$ \n",
    "\\hat{u} = \\left[ {\\begin{array}{ccc}\n",
    "   0 & -u_3 & u_2 \\\\\n",
    "   u_3 & 0 & -u_1 \\\\\n",
    "   -u_2 & u_1 & 0 \\\\\n",
    "  \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "- $\\hat{u}$ is a linear operator that denotes cross-product in $R^3$, $\\hat{u}v = u \\times v$ where $u = [u_1, u_2, u_3]^T$.\n",
    "- Rank of $\\hat{u}$ is 2. Null space of $\\hat{u}$ is spanned by $u$ as $\\hat{u}u = u^T\\hat{u} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Singular value decomposition\n",
    "\n",
    "- SVD can very compactly represent what a matrix does to a vector space.\n",
    "- Generalization of eigenvalues and eigenvectors to non-square matrices (Singular values of $A$ are related to eigenvalues of $A^TA$).\n",
    "- Useful for solving matrix inversion, rank computation, linear least squares estimation, fixed-rank approximation and projections.\n",
    "\n",
    "Let $A \\in R^{m \\times n}$ with $m \\geq n$ with $rank(A) = p$. Then there exists\n",
    "\n",
    "- $U \\in R^{m \\times p}$ whose columns are orthonormal\n",
    "- $V \\in R^{n \\times p}$ whose columns are orthonormal\n",
    "- $\\Sigma \\in R^{p \\times p}$ = $diag\\{\\sigma_1, \\ldots, \\sigma_p\\}$ where $\\sigma_1 \\geq \\ldots \\geq \\sigma_p$\n",
    "\n",
    "such that $A = U \\Sigma V^T$.\n",
    "\n",
    "This generalizes eigenvalue decomposition which can be used to decompose only square, aymmetric matrices while SVD can be used to decompose any non-square matrix with rank $p$. \n",
    "\n",
    "- Singular values $\\sigma_i$ of A are square root of eigenvalues $\\sigma_i^2$ of $A^TA$.\n",
    "- Geometric interpretation: Matrix $A$ maps an unit sphere into an ellipsoid with semi-axes $\\sigma_i u_i$.\n",
    "\n",
    "#### Generalized (Moore Penrose) Inverse\n",
    "- Useful for solving linear system of equations $Ax = b$ where $A \\in R^{m \\times n}$, i.e. $A$ is not square or invertible.\n",
    "- If $SVD(A) = U \\Sigma V^T$, then the psuedoinverse, $A^{\\dagger} = V \\Sigma^{\\dagger} U^T$ where\n",
    "$$ \n",
    "\\Sigma^{\\dagger} = \\left[ {\\begin{array}{cc}\n",
    "   \\Sigma_1^{-1} & 0 \\\\\n",
    "   0 & 0 \\\\\n",
    "  \\end{array} } \\right]\n",
    "$$\n",
    "and $\\Sigma_1$ is the diagonal matrix of nonzero singular values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
